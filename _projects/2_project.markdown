---
layout: page
title: Microsoft: Project Brainwave 
description: I interned at Microsoft as a Firmware Engineer for Project Brainwave
img:
importance: 2
category: technical
---

**Technologies learned:** Sparse transformer (Natural Language Processing)\
**Tools/Programming languages used:** Python, C\ 

In the Summer of 2019, I was fortunate to intern at Microsoft as a firmware development for Project Brainwave. My goal was to research the sparse transformer NLP model, simulate it and evaluate its performance for inference for Bing.

<hr>

## Project Brainwave 

Project Brainwave is a FPGA based deep learning platform, which accelerates deep neural network (DNN) inferencing for application in natural language processing and computer vision. 

Learn more about the group <a href = "https://www.microsoft.com/en-us/research/project/project-brainwave/"> here </a>

<hr>

## Sparse Transformer 

A sparse transformer is a transformer based NLP which uses sparse factorizations of the attention matrix. Traditional transformer models, while powerful, require time and memory that grows quadratically with sequence length. Sparse transformers were introduced to address this limitation by selecting the most relevant information in the context. 
