---
layout: page
title: Machine Learning for Cyber Security
description: Research Project
img:
importance: 1
category: machine learning
---

In this study, we wanted to optimize the detection of advanced persistent threats (APTs), which are stealthy and hard to detect cyber attacks. To do so, modeled the interaction between APTs and Dynamic Information Flow Tracking (DIFT) defense mechanism as a dynamic game (from game theory). The game involves selecting processes in the system to perform security analysis, which affects the efficiency and resource usage of detection. To solve the game, we proposed a supervised learning-based approach to learn an approximate Nash equilibrium (NE) and demonstrated its convergence using real-world data.

# Goal
Implementation and operation of DIFT, however, introduce memory and performance overhead on the system as it involves tracking and analyzing a large number of benign flows. Thus an optimal selection of processes in the system to perform security analysis is critical for effective and resource efficient detection. Therefore, the goal for this project was to model a cost-effective DIFT-based defense mechanism against APTs that:
- Captures the trade-off between detection accuracy and resource efficiency.
- Accounts for rate of false negatives.
<hr>

# Approach
- Model the DIFT and APT interactions as a stochastic non-zero sum game with imperfect and incomplete information
- Use input convex neural networks (ICNN) to approximate the Nash equilibrium of the game.

# Game
The modeled game as two players, the attacker (A) and the defender (D). 
- A's goal is to evade detection and reach its target. Therefore, A gets rewarded for reaching its target and penalized for getting detected or dropping out of the system.
- D's goal is to dynamically place traps on A's path in a way that maximizes the probability of detection and minimizes system overhead. Therefore, D gets rewarded for detecting A or if A drops out and gets penalized if A is able to reach its target without detection. D also incurs a constant resource cost for using system resources.

The game takes place on an information flow graph with nodes and edges.

# Solution Concept
We proved that the above game:
- terminates in a finite number of steps
- has a Nash equilibrium
- has complexity linear in the number of stages and edges for computing the transition probabilities

The key challenge is to find the Nash equilibrium (NE) for the above game with unknown transition probabilities. The transition probabilities are unknown because of the unknown rate of false negatives in the DIFT system. If transition probabilities are not fully known, it becomes difficult to solve for the NE as the utility function (reward) becomes hard to compute. Therefore, we use supervised learning to approximate the NE. 

# Supervised Learning 
The intuition behind using supervised learning is to train two neural networks, one for A and one for D to predict the utility function (reward) for A and D respectively. The training is done by an alternating optimization method, where each player's strategy is updated by fixing the strategy of the opponent and maximizing the utility funtion of that player. Once the two neural networks converge, the resultant utilities of each player is the utility at the approximate NE. 

## Why use input convex neural networks?
To solve for the NE, we use a specific type of neural network known as the partially input convex neural networks (PICNN). PICNNs have constraints on the parameters, such that the output of the network is a convex function of a subset of the inputs. We use PICNNs because the utility functions of A and D are non-concave with respect to the transition probabilities. Therefore, there is a chance that using a normal neural network will converge to a local NE. To get around this, we use PICNNs to get a convex relaxation of the utility functions.

